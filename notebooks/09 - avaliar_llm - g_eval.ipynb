{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1eff2589-e61e-4953-a974-28466d8ed1fe",
   "metadata": {},
   "source": [
    "# Avaliação automatizada de LLMs utilizando DeepEval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d2abb5-5184-40cc-b384-55cc8a0b2623",
   "metadata": {},
   "source": [
    "Atenção: \n",
    "\n",
    "Executar os comandos abaixo no prompt antes de iniciar os testes:\n",
    "\n",
    "```bash\n",
    ".venv\\Scripts\\activate\n",
    "deepeval set-local-model \\\n",
    "         --model-name=llama3.2:latest \\\n",
    "         --base-url=\"http://localhost:11434/v1/\" \\\n",
    "         --api-key=\"ollama\"\n",
    "```\n",
    "\n",
    "ref.: https://docs.confident-ai.com/docs/metrics-llm-evals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "866d646b-755c-41b6-9ed4-eca8433720da",
   "metadata": {},
   "source": [
    "deepeval set-local-model --model-name=phi3:medium --base-url=\"http://localhost:11434/v1/\"--api-key=\"ollama\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a828c71-caf7-4437-9345-4f499b339b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepeval.test_case import LLMTestCase\n",
    "from deepeval.test_case import LLMTestCaseParams\n",
    "from deepeval.metrics import GEval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4498eabd-6b8a-4f9d-af4a-21fb1a44a165",
   "metadata": {},
   "source": [
    "## Corretude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce997bf-bc1b-4ff7-8551-62b8896ec272",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a métrica GEval para avaliar a \"Corretude\" da saída do LLM\n",
    "metric = GEval(\n",
    "    name=\"Correctness\",  # Nome da métrica, que neste caso avalia a exatidão factual da resposta\n",
    "    criteria=\"Determine if the current output is factually correct based on the expected output.\",  # Descreve o objetivo da métrica\n",
    "    evaluation_steps=[\n",
    "        # Passos para a avaliação:\n",
    "        \"Check if any facts in the 'current output' contradict any facts in the 'expected output'.\",  # Checa contradições entre a resposta e o esperado\n",
    "        \"Heavily penalize the omission of important details.\",  # Define uma penalidade maior para omissões importantes\n",
    "        \"Vague language or contradictory opinions are not acceptable.\"  # Explicita que linguagem vaga afeta a avaliação\n",
    "    ],\n",
    "    # Parâmetros a serem usados na avaliação (entrada, saída atual e saída esperada)\n",
    "    evaluation_params=[LLMTestCaseParams.INPUT, LLMTestCaseParams.ACTUAL_OUTPUT, LLMTestCaseParams.EXPECTED_OUTPUT],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a2891f-0a57-4713-b4ad-0b0c5cc3f066",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define um caso de teste que contém a entrada, a saída gerada e a saída esperada para a comparação\n",
    "test_case = LLMTestCase(\n",
    "    input=\"Who was the first president of Brazil?\",  # Entrada fornecida ao modelo\n",
    "    actual_output=\"The first president of Brazil was Deodoro da Fonseca.\",  # Resposta gerada pelo modelo\n",
    "    expected_output=\"The first president of Brazil was Deodoro da Fonseca.\"  # Resposta correta esperada\n",
    ")\n",
    "\n",
    "# Aplica a métrica ao caso de teste e realiza a avaliação\n",
    "metric.measure(test_case)\n",
    "\n",
    "# Exibe o score calculado pela métrica, que representa o nível de corretude factual da saída\n",
    "print(f\"Score: {metric.score}\")\n",
    "# Exibe a justificativa para o score atribuído, explicando eventuais discrepâncias\n",
    "print(f\"Reason: {metric.reason}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7dc349-447c-4988-aaf6-0c64b2a704b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_case = LLMTestCase(\n",
    "    input=\"Who was the first president of Brazil?\",  # Entrada fornecida ao modelo\n",
    "    actual_output=\"It was Deodoro da Fonseca.\",  # Resposta gerada pelo modelo\n",
    "    expected_output=\"The first president of Brazil was Deodoro da Fonseca.\"  # Resposta correta esperada\n",
    ")\n",
    "\n",
    "metric.measure(test_case)\n",
    "\n",
    "print(f\"Score: {metric.score}\")\n",
    "print(f\"Reason: {metric.reason}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eda548e-74bb-4c8a-b1e1-2b4927756914",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_case = LLMTestCase(\n",
    "    input=\"Who was the first president of Brazil?\", \n",
    "    actual_output=\"It was the singer Roberto Carlos da Fonseca!\",\n",
    "    expected_output=\"The first president of Brazil was Deodoro da Fonseca.\" \n",
    ")\n",
    "\n",
    "metric.measure(test_case)\n",
    "\n",
    "print(f\"Score: {metric.score}\")\n",
    "print(f\"Reason: {metric.reason}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54867b0f-54d6-4990-b057-bb28dfd1184f",
   "metadata": {},
   "source": [
    "## Coerência"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58cbf368-d36c-4aab-a061-5259f5b6295f",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = GEval(\n",
    "    name=\"Coherence\", \n",
    "    criteria=\"Evaluate if the current output is logically consistent and maintains a clear sequence of ideas.\",\n",
    "    evaluation_steps=[\n",
    "        \"Ensure the response maintains a logical progression of thoughts.\",\n",
    "        \"Identify any contradictions or deviations in the flow of information.\",\n",
    "        \"Consider if the response is understandable and follows a cohesive structure.\"\n",
    "    ],\n",
    "    evaluation_params=[LLMTestCaseParams.INPUT, LLMTestCaseParams.ACTUAL_OUTPUT],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745290bc-02f9-4744-9fa5-effdff839999",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_case = LLMTestCase(\n",
    "    input=\"Explain the process of photosynthesis.\",\n",
    "    actual_output=(\n",
    "        \"Photosynthesis is the process by which plants convert sunlight into chemical energy. \"\n",
    "        \"First, plants absorb water through their roots and carbon dioxide through their leaves. \"\n",
    "        \"Sunlight is captured by chlorophyll in the chloroplasts, initiating a reaction that produces glucose and oxygen. \"\n",
    "        \"This process is essential for the production of oxygen and is a crucial part of the Earth’s ecosystem.\"\n",
    "    )\n",
    ")\n",
    "\n",
    "metric.measure(test_case)\n",
    "\n",
    "print(f\"Score: {metric.score}\")\n",
    "print(f\"Reason: {metric.reason}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc0032b-106d-46e2-abb9-18e3c1520b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_case = LLMTestCase(\n",
    "    input=\"Explain the process of photosynthesis.\",\n",
    "    actual_output=(\n",
    "        \"Photosynthesis is the process where plants absorb sunlight and produce sugar. \"\n",
    "        \"In this process, animals also play a role in capturing sunlight. \"\n",
    "        \"This helps plants release water into the atmosphere. \"\n",
    "        \"Overall, photosynthesis is about how plants make use of moonlight to grow faster.\"\n",
    "    )\n",
    ")\n",
    "\n",
    "metric.measure(test_case)\n",
    "\n",
    "print(f\"Score: {metric.score}\")\n",
    "print(f\"Reason: {metric.reason}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb1f614-9e85-4fe1-9e15-7164af2a42fc",
   "metadata": {},
   "source": [
    "## Relevância"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69de26af-2259-4ac0-98fc-6fae8ec6b2fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = GEval(\n",
    "    name=\"Relevance\",\n",
    "    criteria=\"Evaluate if the current output is directly related to the main subject of the question or context provided.\",\n",
    "    evaluation_steps=[\n",
    "        \"Ensure the response directly addresses the main subject of the question, specifically looking for a clear answer to 'What is the capital of France?'.\",\n",
    "        \"Identify if any extraneous or unrelated information is included.\",\n",
    "        \"Penalize responses that provide background information without answering the question directly.\"\n",
    "    ],\n",
    "    evaluation_params=[LLMTestCaseParams.INPUT, LLMTestCaseParams.ACTUAL_OUTPUT],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42168e9-0f21-45ce-a2fc-de2912f6023d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_case = LLMTestCase(\n",
    "    input=\"What is the capital of France?\",\n",
    "    actual_output=\"The capital of France is Paris.\"\n",
    ")\n",
    "\n",
    "metric.measure(test_case)\n",
    "\n",
    "print(f\"Score: {metric.score}\")\n",
    "print(f\"Reason: {metric.reason}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8dfd696-e4d7-4931-9faf-664a42a96332",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_case = LLMTestCase(\n",
    "    input=\"What is the capital of France?\",\n",
    "    actual_output=\"Buenos Aires is known for its rich culture and famous landmarks... \"\n",
    "                  \"and some say it looks a lot like France\"\n",
    ")\n",
    "\n",
    "metric.measure(test_case)\n",
    "\n",
    "print(f\"Score: {metric.score}\")\n",
    "print(f\"Reason: {metric.reason}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "828499c8-b7a2-4ddd-8977-e34826724d5f",
   "metadata": {},
   "source": [
    "## Fluência"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "813b1375-fcdf-4cfd-8df7-59176f28c7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = GEval(\n",
    "    name=\"Fluency\",\n",
    "    criteria=\"Evaluate if the current output is grammatically correct, well-structured, and uses natural language.\",  # Descreve o objetivo da métrica\n",
    "    evaluation_steps=[\n",
    "        \"Check if the response is free of grammatical errors; if errors exist, identify them.\",  # Verifica e identifica erros gramaticais\n",
    "        \"Ensure the sentence structure is clear and well-formed; specify any issues with structure.\",  # Garante que a estrutura da frase é clara, especificando problemas\n",
    "        \"Assess if the language sounds natural and fluent; indicate any awkward phrasing or unnatural language.\"  # Identifica frases ou construções pouco naturais\n",
    "    ],\n",
    "    evaluation_params=[LLMTestCaseParams.ACTUAL_OUTPUT]\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb35cdf-aef3-4de0-be27-9d3658f0afc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_case = LLMTestCase(\n",
    "    input=\"Explain the purpose of photosynthesis in plants.\",\n",
    "    actual_output=\"Photosynthesis allows plants to convert sunlight into energy. Through this process, plants produce oxygen and glucose, which they use for growth and energy storage. This process is essential for plant survival and supports life on Earth by producing oxygen.\"\n",
    ")\n",
    "metric.measure(test_case)\n",
    "\n",
    "print(f\"Score: {metric.score}\")\n",
    "print(f\"Reason: {metric.reason}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2625601-70ee-440b-bfe1-e81732ddcce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_case = LLMTestCase(\n",
    "    input=\"Explain the purpose of photosynthesis in plants.\",\n",
    "    actual_output=\"Photosynthesis are important it lets plant make food from sun. Plant make oxygen, food by sunlight and water. That very important for Earth.\"\n",
    ")\n",
    "metric.measure(test_case)\n",
    "\n",
    "print(f\"Score: {metric.score}\")\n",
    "print(f\"Reason: {metric.reason}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6466df06-8b3c-4b9c-bb5b-1c18be66ff4b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
